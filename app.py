import streamlit as st
import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="æ­´å²äººç‰©AI", page_icon="ğŸ“œ")

st.title("ğŸ“œ æ­´å²äººç‰©AIãƒãƒ£ãƒƒãƒˆ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šAPIã‚­ãƒ¼ã®è¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨ã€‚ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã¯Secretsã‚’ä½¿ã„ã¾ã™ï¼‰
with st.sidebar:
    st.header("è¨­å®š")
    st.markdown("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãã®å†…å®¹ã‚’å­¦ç¿’ã—ã¦è³¢ããªã‚Šã¾ã™ã€‚")
    uploaded_file = st.file_uploader("å­¦ç¿’ã•ã›ã‚‹PDF", type="pdf")

# APIã‚­ãƒ¼ã®å–å¾—ï¼ˆStreamlit Secrets ã¾ãŸã¯ ç’°å¢ƒå¤‰æ•°ï¼‰
openai_api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
pinecone_api_key = st.secrets.get("PINECONE_API_KEY") or os.getenv("PINECONE_API_KEY")
pinecone_index_name = "history-chat" # Pineconeã§ä½œã£ãŸIndexå

if not openai_api_key or not pinecone_api_key:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ç’°å¢ƒå¤‰æ•°ã®ã‚»ãƒƒãƒˆ
os.environ["OPENAI_API_KEY"] = openai_api_key
os.environ["PINECONE_API_KEY"] = pinecone_api_key

# --- 1. PDFã®å­¦ç¿’å‡¦ç†ï¼ˆè³¢ããªã‚‹éƒ¨åˆ†ï¼‰ ---
if uploaded_file is not None:
    with st.spinner("è³‡æ–™ã‚’èª­ã¿è¾¼ã‚“ã§è¨˜æ†¶ã—ã¦ã„ã¾ã™..."):
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        temp_pdf_path = "temp_uploaded.pdf"
        with open(temp_pdf_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # PDFèª­ã¿è¾¼ã¿ã¨åˆ†å‰²
        loader = PyPDFLoader(temp_pdf_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.split_documents(documents)
        
        # Pineconeã¸ä¿å­˜ï¼ˆæ°¸ç¶šè¨˜æ†¶ï¼‰
        embeddings = OpenAIEmbeddings()
        PineconeVectorStore.from_documents(docs, embeddings, index_name=pinecone_index_name)
        
        st.success(f"{len(docs)} ãƒšãƒ¼ã‚¸åˆ†ã®çŸ¥è­˜ã‚’ç²å¾—ã—ã¾ã—ãŸï¼")
        os.remove(temp_pdf_path) # æƒé™¤

# --- 2. ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã®æº–å‚™ ---
@st.cache_resource
def get_chat_chain():
    # Pineconeã‹ã‚‰çŸ¥è­˜ã‚’å–ã‚Šå‡ºã™è¨­å®š
    embeddings = OpenAIEmbeddings()
    vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)
    retriever = vectorstore.as_retriever()
    
    # AIã®è¨­å®šï¼ˆæ­´å²ä¸Šã®äººç‰©ã«ãªã‚Šãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã“ã“ã§èª¿æ•´ï¼‰
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7)
    
    memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True,
        output_key="answer"
    )
    
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        return_source_documents=False,
        verbose=True
    )
    return chain

chain = get_chat_chain()

# --- 3. ãƒãƒ£ãƒƒãƒˆç”»é¢ã®è¡¨ç¤º ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "æŸï¼ˆãã‚ŒãŒã—ï¼‰ã«ä½•ã‹ç”¨ã‹ï¼Ÿ"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›...")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("æ€æ¡ˆä¸­..."):
            response = chain.invoke({"question": prompt})
            answer = response["answer"]
            st.write(answer)
            
    st.session_state.messages.append({"role": "assistant", "content": answer})